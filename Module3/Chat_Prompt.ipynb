{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOoC+Pa/qitjCDcmexwCce5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bforoura/GENAI26/blob/main/Module3/Chat_Prompt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# **The Chat Blueprint and the Fake Brain**\n",
        "\n",
        "---\n",
        "* Here, we set up \"Chat\" Blueprint.\n",
        "\n",
        "* Instead of just sending a single sentence to the AI, we create a **ChatPromptTemplate** which organizes information into three distinct layers:\n",
        "\n",
        "  1. **The System Layer**: It defines the AI's \"**personna Instructions**\"; e.g., \"**You are a professional assistant**\"\n",
        "\n",
        "  2. **The History Layer**: It leaves an **empty placeholder** in the middle of the conversation.\n",
        "\n",
        "  3. **The Human Layer**: It puts the current request at the very end.\n",
        "---\n",
        "\n",
        "* The **Fake Brain or FakeListLLM** is essentially a **proxy** for a real AI like GPT-4 or Gemini.\n",
        "\n",
        "* When building a complex chain, we often want to make sure the **plumbing** of the code works, i.e., the instructions are formatted correctly and the history is being passed properly without spending money or waiting for a real AI to generate a response.\n",
        "\n",
        "* Here is how the **Fake Brain** functions:\n",
        "\n",
        "  * Instead of using billions of parameters to think of an original answer, the Fake Brain takes a list of strings that you provide when you create it.\n",
        "\n",
        "  * The first time the chain runs, it gives you the first string in your list.\n",
        "\n",
        "  * The second time it runs, it gives you the second string.\n",
        "\n",
        "* Even though **it isn't thinking**, it still acts like a **Runnable**.\n",
        "* This means it can sit perfectly in the middle of your assembly line:\n",
        "\n",
        "       `Template | Fake Brain | Parser `\n",
        "\n",
        "* The Template doesn't know the brain is fake; it just sends its list of messages to it.\n",
        "\n",
        "* The Fake Brain ignores the content of those messages and just hands back the next pre-set answer in its folder.\n",
        "\n"
      ],
      "metadata": {
        "id": "ikPEp-mQQaJA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain langchain-community"
      ],
      "metadata": {
        "id": "iEiCnyO4P56F"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from langchain_community.llms.fake import FakeListLLM\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "\n",
        "# Create a \"Fake\" Brain\n",
        "# It will return these answers in order each time you run the chain\n",
        "responses = [\"The job description looks great!\", \"I remember you mentioned a restaurant.\"]\n",
        "llm = FakeListLLM(responses=responses)\n",
        "\n",
        "\n",
        "\n",
        "# Create the Modern Chat Chain\n",
        "# Use 'System', 'History', and 'Human'\n",
        "chat_prompt_template = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a professional assistant.\"),\n",
        "    (\"placeholder\", \"{history}\"),\n",
        "    (\"human\", \"Analyze: {job_description}\")\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "# The assembly line\n",
        "chain = chat_prompt_template | llm | StrOutputParser()\n",
        "\n",
        "\n",
        "\n",
        "# Run the experiment\n",
        "my_history = [(\"human\", \"Hi!\"), (\"ai\", \"Hello!\")]\n",
        "result = chain.invoke({\"job_description\": \"Developer\", \"history\": my_history})\n",
        "\n",
        "print(f\"Fake AI Response: {result}\")\n",
        "\n",
        "\n",
        "\n",
        "# Verify the logic\n",
        "# This shows how the template 'unpacked' the history into a list of 4 messages\n",
        "prompt_data = chat_prompt_template.invoke({\"job_description\": \"Developer\", \"history\": my_history})\n",
        "print(f\"Total messages in the chain: {len(prompt_data.messages)}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2wRF2tLJQAFw",
        "outputId": "bafc079b-8de9-4637-f0e3-089fbadb9ed3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fake AI Response: The job description looks great!\n",
            "Total messages in the chain: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Message Expansion**\n",
        "\n",
        "* The above output proves that the **Placeholder** successfully unpacked the history.\n",
        "\n",
        "  1. **Message 1 (System)**: \"You are a professional assistant.\" This was defined in the template.\n",
        "\n",
        "  2. **Message 2 (Human History)**: \"Hi!\". This was pulled from my_history list\n",
        "\n",
        "  3. **Message 3 (AI History)**: \"Hello!\" . This was pulled from my_history list\n",
        "\n",
        "  4. **Message 4 (Current Human)**: \"Analyze: Developer\". This was created by combining the template with the input"
      ],
      "metadata": {
        "id": "ClV3g1k6TG4N"
      }
    }
  ]
}