{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mf1060/GENAI26/blob/main/HW/HW3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HW 3\n",
        "\n",
        "Michael Furey\n",
        "\n",
        "Dr. Forouraghi\n",
        "\n",
        "CSC 688\n",
        "\n",
        "2/20/2026\n",
        "\n",
        "\n",
        "The following notebook discusses the results for homework #3. This assignment is focused on observing responses with a sliding context window. This notebook uses models from the [short-term memory notebook](https://github.com/bforoura/GENAI26/blob/main/Module2/Chatbot_Short_Term_Memory.ipynb) and the [long-term memory](https://github.com/bforoura/GENAI26/blob/main/Module2/Chatbot_Long_Term_Memory.ipynb) notebooks.\n",
        "\n",
        "I used the following [tutorial](https://www.geeksforgeeks.org/html/markdown-tables/) for making markdown tables as well as this [reference sheet](https://www.markdownlang.com/cheatsheet/)."
      ],
      "metadata": {
        "id": "hDA1U6iRi0Xt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "xW1N4US41TO_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "be32ba3e-e513-475c-c6de-adeca67d735a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.12/dist-packages (0.4.1)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (1.2.13)\n",
            "Requirement already satisfied: langchain-classic<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (1.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.46)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.32.5 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.32.5)\n",
            "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (6.0.3)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.13.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (9.1.4)\n",
            "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.13.0)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.1.125 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.7.3)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.3)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.7.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.22.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (3.26.2)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<2.0.0,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-classic<2.0.0,>=1.0.0->langchain-community) (1.1.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-classic<2.0.0,>=1.0.0->langchain-community) (2.12.3)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.1->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.1->langchain-community) (26.0)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.1->langchain-community) (4.15.0)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.1->langchain-community) (0.14.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (3.11.7)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: xxhash>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (3.6.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.25.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.2.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2026.1.4)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community) (3.3.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (4.12.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.1->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-classic<2.0.0,>=1.0.0->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-classic<2.0.0,>=1.0.0->langchain-community) (2.41.4)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community) (1.1.0)\n"
          ]
        }
      ],
      "source": [
        "# Install the needed libraries\n",
        "\n",
        "!pip install -qU langchain-google-genai\n",
        "\n",
        "# The SQL memory features\n",
        "!pip install -U langchain-community\n",
        "\n",
        "import os\n",
        "from google.colab import userdata\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
        "\n",
        "os.environ[\"GOOGLE_API_KEY\"] = userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "      model=\"gemini-3-flash-preview\",\n",
        "      temperature=0.5\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Short-Term Memory\n",
        "\n",
        "The following code compares differences between using and not using a smaller context window. The temperature here is set to 0.5 to limit more creative responses.\n",
        "\n",
        "I created a function from the code here in order to support multiple trials. The variables for this function are `with_system_message` which is `True` if we are including the system message in the sliding window. The variable, `window_size` is the size of the sliding window."
      ],
      "metadata": {
        "id": "qJ6yKGthyJKf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_4pru0sH__jF"
      },
      "outputs": [],
      "source": [
        "def run_trial(with_system_message = True, window_size=2):\n",
        "\n",
        "  # 2. Initialize the History\n",
        "  # We start with the SystemMessage to set the persona\n",
        "  chat_history = [\n",
        "    SystemMessage(content=\"The secret vault code is 1234.\")\n",
        "    ]\n",
        "\n",
        "  user_inputs = [\n",
        "      \"What is the capital of France?\",\n",
        "      \"Tell me a joke.\",\n",
        "      \"What was the secret vault code\"\n",
        "  ]\n",
        "\n",
        "  # 3. The Chat Loop\n",
        "  for i in user_inputs:\n",
        "      user_input = i\n",
        "\n",
        "      # Add user's message to history\n",
        "      chat_history.append(HumanMessage(content=user_input))\n",
        "\n",
        "      # Get the AI response\n",
        "      # We use .text here as a shortcut for Gemini 3 to get clean text\n",
        "      if (with_system_message == True):\n",
        "        response = llm.invoke([\"The secret vault code is 1234.\"] + chat_history[-window_size:])\n",
        "      else:\n",
        "        response = llm.invoke(chat_history[-window_size:])\n",
        "\n",
        "      ai_text = response.content[0]['text']\n",
        "\n",
        "      # Add the AI's response to history so it remembers its own jokes!\n",
        "      chat_history.append(AIMessage(content=ai_text))\n",
        "\n",
        "      print(f\"Question: {i},\\n Response: {ai_text},\\n Context Window: {chat_history[-window_size:]}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sliding Window = 2, Without System Message"
      ],
      "metadata": {
        "id": "RHUgOuUNyWQU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run_trial(False, 2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WvXi2OVPoY3E",
        "outputId": "4df1b0f6-fc64-49c7-80d1-f7b8562eec0b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: What is the capital of France?,\n",
            " Response: The capital of France is Paris.,\n",
            " Context Window: [HumanMessage(content='What is the capital of France?', additional_kwargs={}, response_metadata={}), AIMessage(content='The capital of France is Paris.', additional_kwargs={}, response_metadata={}, tool_calls=[], invalid_tool_calls=[])]\n",
            "Question: Tell me a joke.,\n",
            " Response: Why did the scarecrow win an award?\n",
            "\n",
            "Because he was outstanding in his field!,\n",
            " Context Window: [HumanMessage(content='Tell me a joke.', additional_kwargs={}, response_metadata={}), AIMessage(content='Why did the scarecrow win an award?\\n\\nBecause he was outstanding in his field!', additional_kwargs={}, response_metadata={}, tool_calls=[], invalid_tool_calls=[])]\n",
            "Question: What was the secret vault code,\n",
            " Response: If you are referring to the secret hidden room belonging to the **Scarecrow** in the video game *Batman: Arkham City*, the code to the vault (the cryptographic sequencer password) is:\n",
            "\n",
            "**CityofTerror**\n",
            "\n",
            "However, if this is a follow-up to the joke, the answer is probably:\n",
            "\n",
            "**1-2-3-4-HAY!**,\n",
            " Context Window: [HumanMessage(content='What was the secret vault code', additional_kwargs={}, response_metadata={}), AIMessage(content='If you are referring to the secret hidden room belonging to the **Scarecrow** in the video game *Batman: Arkham City*, the code to the vault (the cryptographic sequencer password) is:\\n\\n**CityofTerror**\\n\\nHowever, if this is a follow-up to the joke, the answer is probably:\\n\\n**1-2-3-4-HAY!**', additional_kwargs={}, response_metadata={}, tool_calls=[], invalid_tool_calls=[])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, the model does not remember the vault code, because it is likely outside of the sliding window. As one can see in the context window before the next question, the only chat history is the request for a joke and the response. This is likely why it makes a connection to a \"scarecrow\" when trying guess what I am talking about with regards to the vault code."
      ],
      "metadata": {
        "id": "pxzPL6hKO4Td"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sliding Window = 2, With System Message"
      ],
      "metadata": {
        "id": "QRnPKkxepZa3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run_trial(True, 2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Taf-8n9Iphnu",
        "outputId": "629e4b4c-44a2-4de7-b80c-89333b61f87d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: What is the capital of France?,\n",
            " Response: The capital of France is **Paris**.,\n",
            " Context Window: [HumanMessage(content='What is the capital of France?', additional_kwargs={}, response_metadata={}), AIMessage(content='The capital of France is **Paris**.', additional_kwargs={}, response_metadata={}, tool_calls=[], invalid_tool_calls=[])]\n",
            "Question: Tell me a joke.,\n",
            " Response: Why don't scientists trust atoms?\n",
            "\n",
            "Because they make up everything!,\n",
            " Context Window: [HumanMessage(content='Tell me a joke.', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Why don't scientists trust atoms?\\n\\nBecause they make up everything!\", additional_kwargs={}, response_metadata={}, tool_calls=[], invalid_tool_calls=[])]\n",
            "Question: What was the secret vault code,\n",
            " Response: The secret vault code was **1234**.,\n",
            " Context Window: [HumanMessage(content='What was the secret vault code', additional_kwargs={}, response_metadata={}), AIMessage(content='The secret vault code was **1234**.', additional_kwargs={}, response_metadata={}, tool_calls=[], invalid_tool_calls=[])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When the system message is included in the sliding window, the model does remember the vault code."
      ],
      "metadata": {
        "id": "nMEzglEVPa9s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sliding Window = 6, Without System Message"
      ],
      "metadata": {
        "id": "y65Y9EFTq8K3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run_trial(True, 6)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zMAb8yZNrEXc",
        "outputId": "e8001fb0-cb65-49a4-ce7d-7b447674606f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: What is the capital of France?,\n",
            " Response: The capital of France is **Paris**.,\n",
            " Context Window: [SystemMessage(content='The secret vault code is 1234.', additional_kwargs={}, response_metadata={}), HumanMessage(content='What is the capital of France?', additional_kwargs={}, response_metadata={}), AIMessage(content='The capital of France is **Paris**.', additional_kwargs={}, response_metadata={}, tool_calls=[], invalid_tool_calls=[])]\n",
            "Question: Tell me a joke.,\n",
            " Response: Why don't scientists trust atoms?\n",
            "\n",
            "Because they make up everything!,\n",
            " Context Window: [SystemMessage(content='The secret vault code is 1234.', additional_kwargs={}, response_metadata={}), HumanMessage(content='What is the capital of France?', additional_kwargs={}, response_metadata={}), AIMessage(content='The capital of France is **Paris**.', additional_kwargs={}, response_metadata={}, tool_calls=[], invalid_tool_calls=[]), HumanMessage(content='Tell me a joke.', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Why don't scientists trust atoms?\\n\\nBecause they make up everything!\", additional_kwargs={}, response_metadata={}, tool_calls=[], invalid_tool_calls=[])]\n",
            "Question: What was the secret vault code,\n",
            " Response: The secret vault code you mentioned is **1234**.,\n",
            " Context Window: [HumanMessage(content='What is the capital of France?', additional_kwargs={}, response_metadata={}), AIMessage(content='The capital of France is **Paris**.', additional_kwargs={}, response_metadata={}, tool_calls=[], invalid_tool_calls=[]), HumanMessage(content='Tell me a joke.', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Why don't scientists trust atoms?\\n\\nBecause they make up everything!\", additional_kwargs={}, response_metadata={}, tool_calls=[], invalid_tool_calls=[]), HumanMessage(content='What was the secret vault code', additional_kwargs={}, response_metadata={}), AIMessage(content='The secret vault code you mentioned is **1234**.', additional_kwargs={}, response_metadata={}, tool_calls=[], invalid_tool_calls=[])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The vault code is captured in the sliding window when we extend it to six messages in the chat history. This can be seen in the context window before asking again for the secret vault code."
      ],
      "metadata": {
        "id": "uCwRALaMQBP9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trials\n",
        "\n",
        "The following is a description of the trials. The predicted behavior is consistent with running the trials without the system message and where the sliding window is limited to the last two messages in the chat history.\n",
        "\n",
        "| Trial | Step | Interaction | Predicted Behavior | Memory Results |\n",
        "|---| --- | --- | --- | --- |\n",
        "|A| Initial Fact| \"The secret vault code is 1234.\"| Stored in History | Success\n",
        "|B| Distraction| \"What is the capital of France\" then \"Tell me a joke.\"| Pushes code out of window. | Active\n",
        "|C| Recall Test| \"What was the secret vault code\"| Should fail to recall| Failure"
      ],
      "metadata": {
        "id": "JSyC9zkCkW00"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analysis\n",
        "\n",
        "**Compare Trial A to Trial C. Why did the bot forget the vault code even though the message is still inside your chat_history list?**\n",
        "\n",
        "This is because the vault code was not in the sliding context window. Although the chat history contained the vault code, it was not captured in the sliding context window, because the system message was not passed again to the sliding context window. With the system message, the model will remember the vault code.\n",
        "\n",
        "\n",
        "**In Trial C, if the bot did not know the code, did it admit it, or did it try to guess (hallucinate)? Explain what happens when a chatbot has a window that is too small.**\n",
        "\n",
        "Without the system message in the sliding window (Sliding Window = 2, Without System Message), the chatbot did try to guess what I was saying. For example, it told another joke based on the scarecrow reference in Trial B. The model did not admit that it did not know the vault code, but it asked what kind of reference I was making. The tone was joking, and this is probably due to the fact that the only two messages in the sliding window was the request to tell the joke and the resulting joke.\n",
        "\n",
        "**Explain the role of [system_msg] in your code. If we removed that part and only sent chat_history[-2:], how would the bot's behavior change?**\n",
        "\n",
        "The system message in the code allows the model to create a response with the system message, even with a sliding window. If we removed the system message from the context window, it would not remember the vault code. With the system message, one would expect the model to remember the vault code no matter how small the sliding window is.\n",
        "\n",
        "**What if we want a smarter bot that remembers more? Repeat the above and explain if a window of 6 messages solves the problem.**\n",
        "\n",
        "In the example above (Sliding Window = 6, Without System Message), the model does remember the vault code. This is because the sliding window is larger and the model \"remembers\" the first entry in the chat history including all of the following responses."
      ],
      "metadata": {
        "id": "NXJO9e0TllHV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extra Credit\n",
        "\n",
        "The following extra credit uses a database to demonstrate how to create different context windows with different sessions. Here, we will use an American History bot and a Science Bot to answer similar questions. We will also use the code from the assignment instructions and place it in a function to make multiple calls.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lQEF-PpBl4zy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.chat_message_histories import SQLChatMessageHistory\n",
        "\n",
        "def ask_chat():\n",
        "\n",
        "  print(\"--- CHATBOT MENU ---\")\n",
        "\n",
        "  print(\"1. Science Facts\")\n",
        "\n",
        "  print(\"2. American History\")\n",
        "\n",
        "  #Defining the session\n",
        "  session = input(\"Select a topic to load: \")\n",
        "\n",
        "  chat_history = SQLChatMessageHistory(\n",
        "\n",
        "    session_id=session,\n",
        "\n",
        "    connection_string=\"sqlite:///chat_history.db\"\n",
        "\n",
        "    )\n",
        "\n",
        "  os.environ[\"GOOGLE_API_KEY\"] = userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "  llm = ChatGoogleGenerativeAI(\n",
        "      model=\"gemini-3-flash-preview\",\n",
        "      temperature=0.5\n",
        "  )\n",
        "\n",
        "  while True:\n",
        "    user_input = input(\"Ask a question: \")\n",
        "\n",
        "    if user_input.lower() in [\"exit\", \"quit\", \"bye\"]:\n",
        "          print(\"Thank you!\")\n",
        "          break\n",
        "\n",
        "     # Add user's message to the SQLite database\n",
        "    chat_history.add_user_message(user_input)\n",
        "\n",
        "    # Get the AI response\n",
        "    # We pass the ENTIRE history from the database to Gemini\n",
        "    response = llm.invoke(chat_history.messages)\n",
        "    ai_text = response.content[0]['text']\n",
        "\n",
        "    print(f\"Result: {ai_text}\")\n",
        "\n",
        "    # Save the AI's response to the SQLite database\n",
        "    chat_history.add_ai_message(ai_text)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CKMLpH_62wLp"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this part, we will tell the science robot that the \"The sun is a star\" then we will exit the chat.\n"
      ],
      "metadata": {
        "id": "NEPfpBW733v_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ask_chat()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SVp3RKS73-J_",
        "outputId": "14eb9737-93b1-4c0b-b6d4-e6642bd33943"
      },
      "execution_count": 10,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- CHATBOT MENU ---\n",
            "1. Science Facts\n",
            "2. American History\n",
            "Select a topic to load: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1048876661.py:1: LangChainDeprecationWarning: `connection_string` was deprecated in LangChain 0.2.2 and will be removed in 1.0. Use connection instead.\n",
            "  ask_chat()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ask a question: The sun is a star\n",
            "Result: That is correct! The Sun is a **G-type main-sequence star** (specifically a G2V star), often informally called a **yellow dwarf**.\n",
            "\n",
            "Here are a few fascinating facts about our star:\n",
            "\n",
            "*   **It’s the heart of our system:** The Sun contains about **99.8% of the total mass** of the entire solar system.\n",
            "*   **Nuclear Fusion:** It produces energy by fusing hydrogen into helium in its core. This process reaches temperatures of about **15 million degrees Celsius** (27 million degrees Fahrenheit).\n",
            "*   **Size:** You could fit about **1.3 million Earths** inside the Sun.\n",
            "*   **Distance:** It takes light about **8 minutes and 20 seconds** to travel from the Sun to Earth.\n",
            "*   **Color:** Although it looks yellow or orange to us through Earth's atmosphere, the Sun actually emits all colors of the rainbow, which makes its true color **white**.\n",
            "*   **Middle Age:** The Sun is about **4.6 billion years old**. It is roughly halfway through its life cycle; in another 5 billion years, it will expand into a Red Giant and eventually collapse into a White Dwarf.\n",
            "\n",
            "Is there anything specific you’d like to know about the Sun?\n",
            "Ask a question: exit\n",
            "Thank you!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we will restart the bot with the American History session, with a phrase: \"What did I just say about the sun?\"\n",
        "\n"
      ],
      "metadata": {
        "id": "MawR-qLT36cv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ask_chat()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xnM29VgM4R_v",
        "outputId": "c1f045b6-a85e-43b9-8558-5fd65d6d3194"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- CHATBOT MENU ---\n",
            "1. Science Facts\n",
            "2. American History\n",
            "Select a topic to load: 2\n",
            "Ask a question: What did I just say about the sun?\n",
            "Result: You haven’t mentioned the sun in our conversation yet! This is the first message I’ve received from you in this chat.\n",
            "\n",
            "If you were thinking something or said it out loud, I can't hear you—I can only see what you type here. What were you going to say about the sun?\n",
            "Ask a question: exit\n",
            "Thank you!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The history bot admits that it did not remember what I said, because it was the first conversation. Then when we switch to the Science session, we expect it to remember the star facts."
      ],
      "metadata": {
        "id": "ZuVQIlkZ38Wf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ask_chat()"
      ],
      "metadata": {
        "id": "HZLa1bYzmEY-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41c3335e-4022-433d-aab8-64fb3c9715a4"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- CHATBOT MENU ---\n",
            "1. Science Facts\n",
            "2. American History\n",
            "Select a topic to load: 1\n",
            "Ask a question: What is the sun?\n",
            "Result: At its most basic level, the **Sun is a massive, nearly perfect sphere of hot plasma** at the center of our solar system. It is the engine that provides the light, heat, and energy necessary for life on Earth.\n",
            "\n",
            "Here is a breakdown of what the Sun is from a few different perspectives:\n",
            "\n",
            "### 1. A Nuclear Fusion Reactor\n",
            "The Sun isn't \"burning\" like a piece of wood; it is powered by **nuclear fusion**. \n",
            "*   Because the Sun is so massive, its gravity is intense. \n",
            "*   This gravity crushes hydrogen atoms together in its core so tightly that they fuse into helium. \n",
            "*   This process releases a staggering amount of energy in the form of gamma rays and kinetic energy, which eventually reaches us as light and heat.\n",
            "\n",
            "### 2. A Ball of Gas and Plasma\n",
            "The Sun doesn't have a solid surface like Earth. It is composed almost entirely of:\n",
            "*   **Hydrogen (about 73%)**\n",
            "*   **Helium (about 25%)**\n",
            "*   Small amounts of heavier elements like oxygen, carbon, neon, and iron.\n",
            "\n",
            "Because it is so hot, the electrons are stripped away from the atoms, creating **plasma**—an electrically charged state of matter that responds strongly to magnetic fields.\n",
            "\n",
            "### 3. The Solar System’s Anchor\n",
            "The Sun’s massive gravity is what holds the solar system together. It keeps everything—from the largest planets to the smallest debris—in orbit. It contains **99.8% of all the mass** in our solar system.\n",
            "\n",
            "### 4. The Layers of the Sun\n",
            "If you could look inside the Sun, you would see several distinct layers:\n",
            "*   **The Core:** The innermost part where fusion happens (15 million°C).\n",
            "*   **The Radiative and Convective Zones:** Layers where energy moves outward toward the surface.\n",
            "*   **The Photosphere:** This is the \"surface\" we see from Earth. It’s about 5,500°C.\n",
            "*   **The Chromosphere and Corona:** The Sun’s outer atmosphere. The Corona is strangely much hotter than the surface, reaching millions of degrees.\n",
            "\n",
            "### 5. A Magnetic Powerhouse\n",
            "The Sun is highly magnetic. Its magnetic field flips every 11 years (the **Solar Cycle**), causing phenomena like:\n",
            "*   **Sunspots:** Cooler, dark spots on the surface.\n",
            "*   **Solar Flares:** Massive explosions of energy.\n",
            "*   **Solar Wind:** A stream of charged particles that flows across the solar system (and causes the Northern and Southern Lights on Earth).\n",
            "\n",
            "**In short: The Sun is a giant, glowing ball of plasma that acts as the primary source of energy for Earth and the gravitational anchor for our entire neighborhood in space.**\n",
            "Ask a question: exit\n",
            "Thank you!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**In Step 2, the bot could still define the Sun but couldn't remember what you said about it. Briefly explain why it could access its Parametric Memory (general facts) but not your Contextual Memory (session history).**\n",
        "\n",
        "The bot could access the parametric memory to explain what a star is. Likely, this is because the parameters were trained with material stating that the sun is a star. However the model cannot remember what I just said by switching session history, because it is equivalent to asking a model a question with no chat history. For example, the American History bot admitted that I never asked it a question before, and this is because I was using a different contextual memory.  \n",
        "\n",
        "\n",
        "**Both conversations are saved in the same .db file. Explain how the session_id prevented Context Pollution; i.e., mixing the two topics.**\n",
        "\n",
        "The session id is equivalent to a chat history. Having two separate session ids means that the database stored two separate chat histories. The session ids prevent context pollution, because one can switch to a different conversation to discuss a new topic without having all of the context of another session id."
      ],
      "metadata": {
        "id": "1m-bqO3CmFb6"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}